{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ağ Anomali Tespiti - Unsupervised Learning Yaklaşımları\n",
    "\n",
    "Bu notebook, KDD Cup 1999 veri seti üzerinde unsupervised learning yöntemleri kullanarak ağ anomali tespiti gerçekleştirir.\n",
    "\n",
    "## Kullanılan Yöntemler:\n",
    "- **K-means Clustering**: Veri noktalarını kümelere ayırarak anomalileri tespit\n",
    "- **DBSCAN**: Yoğunluk tabanlı kümeleme ile outlier tespiti\n",
    "- **Isolation Forest**: Anomali tespiti için özel tasarlanmış algoritma\n",
    "- **One-Class SVM**: Tek sınıf sınıflandırması ile anomali tespiti\n",
    "- **Local Outlier Factor (LOF)**: Yerel yoğunluk tabanlı anomali tespiti\n",
    "\n",
    "## Hedefler:\n",
    "1. Farklı unsupervised algoritmaları karşılaştırmak\n",
    "2. Anomali skorlarını görselleştirmek\n",
    "3. Kümeleme sonuçlarını analiz etmek\n",
    "4. En etkili yöntemi belirlemek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri import et\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Görselleştirme ayarları\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Veri Yükleme ve Ön İşleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri yükleme\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# KDD Cup 1999 kolon isimleri\n",
    "KDD_COLS = [\n",
    "    'duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent',\n",
    "    'hot','num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root',\n",
    "    'num_file_creations','num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login',\n",
    "    'count','srv_count','serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate',\n",
    "    'diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count','dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_srv_diff_host_rate','dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate','label'\n",
    "]\n",
    "\n",
    "# Veriyi yükle\n",
    "print(\"Veri yükleniyor...\")\n",
    "data_path = Path('../data/kddcup.data_10_percent.gz')\n",
    "df = pd.read_csv(data_path, names=KDD_COLS, header=None, compression='gzip')\n",
    "\n",
    "print(f\"Toplam veri boyutu: {df.shape}\")\n",
    "\n",
    "# Özellikleri ve etiketleri ayır\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Eğitim seti boyutu: {X_train.shape}\")\n",
    "print(f\"Test seti boyutu: {X_test.shape}\")\n",
    "\n",
    "# Binary etiketler oluştur (0: normal, 1: anomali)\n",
    "y_train_binary = (y_train != 'normal.').astype(int)\n",
    "y_test_binary = (y_test != 'normal.').astype(int)\n",
    "\n",
    "print(f\"Eğitim setinde normal: {sum(y_train_binary == 0)}, anomali: {sum(y_train_binary == 1)}\")\n",
    "print(f\"Test setinde normal: {sum(y_test_binary == 0)}, anomali: {sum(y_test_binary == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri ön işleme\n",
    "print(\"Veri ön işleme yapılıyor...\")\n",
    "\n",
    "# Kategorik değişkenleri encode et\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Kategorik kolonları belirle\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "\n",
    "# Label encoding\n",
    "le_dict = {}\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Tüm benzersiz değerleri birleştir\n",
    "    all_values = pd.concat([X_train[col], X_test[col]]).unique()\n",
    "    le.fit(all_values)\n",
    "    \n",
    "    X_train_encoded[col] = le.transform(X_train[col])\n",
    "    X_test_encoded[col] = le.transform(X_test[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "print(\"Kategorik değişkenler encode edildi.\")\n",
    "\n",
    "# Sabit kolonları kaldır\n",
    "constant_cols = X_train_encoded.columns[X_train_encoded.nunique() <= 1]\n",
    "if len(constant_cols) > 0:\n",
    "    print(f\"Sabit kolonlar kaldırılıyor: {list(constant_cols)}\")\n",
    "    X_train_encoded = X_train_encoded.drop(columns=constant_cols)\n",
    "    X_test_encoded = X_test_encoded.drop(columns=constant_cols)\n",
    "\n",
    "print(f\"İşlenmiş veri boyutu: {X_train_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi normalize et\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "print(\"Veri normalizasyonu tamamlandı.\")\n",
    "\n",
    "# Anomali tespiti için sadece normal verileri kullan (unsupervised)\n",
    "normal_indices = y_train_binary == 0\n",
    "X_train_normal = X_train_scaled[normal_indices]\n",
    "\n",
    "print(f\"Normal veri boyutu (eğitim için): {X_train_normal.shape}\")\n",
    "\n",
    "# Performans değerlendirmesi için küçük bir subset kullan\n",
    "np.random.seed(42)\n",
    "sample_size = min(10000, len(X_test_scaled))\n",
    "sample_indices = np.random.choice(len(X_test_scaled), sample_size, replace=False)\n",
    "X_test_sample = X_test_scaled[sample_indices]\n",
    "y_test_sample = y_test_binary.iloc[sample_indices]\n",
    "\n",
    "print(f\"Test sample boyutu: {X_test_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Boyut Azaltma ve Görselleştirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA ile boyut azaltma\n",
    "print(\"PCA ile boyut azaltma yapılıyor...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_normal[:5000])  # Hız için subset\n",
    "X_test_pca = pca.transform(X_test_sample)\n",
    "\n",
    "print(f\"PCA açıklanan varyans oranı: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Toplam açıklanan varyans: {pca.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri dağılımını görselleştir\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PCA sonuçları\n",
    "axes[0].scatter(X_test_pca[y_test_sample == 0, 0], X_test_pca[y_test_sample == 0, 1], \n",
    "               alpha=0.6, label='Normal', s=20)\n",
    "axes[0].scatter(X_test_pca[y_test_sample == 1, 0], X_test_pca[y_test_sample == 1, 1], \n",
    "               alpha=0.6, label='Anomali', s=20)\n",
    "axes[0].set_title('PCA - Veri Dağılımı')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Özellik önemlerini göster\n",
    "feature_importance = np.abs(pca.components_).mean(axis=0)\n",
    "top_features = np.argsort(feature_importance)[-10:]\n",
    "\n",
    "axes[1].barh(range(len(top_features)), feature_importance[top_features])\n",
    "axes[1].set_yticks(range(len(top_features)))\n",
    "axes[1].set_yticklabels([X_train_encoded.columns[i] for i in top_features])\n",
    "axes[1].set_title('En Önemli Özellikler (PCA)')\n",
    "axes[1].set_xlabel('Önem Skoru')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal küme sayısını bul (Elbow method)\n",
    "print(\"K-means için optimal küme sayısı belirleniyor...\")\n",
    "\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Hız için daha küçük sample\n",
    "sample_normal = X_train_normal[:3000]\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(sample_normal)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(sample_normal, labels))\n",
    "\n",
    "# Sonuçları görselleştir\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(k_range, inertias, 'bo-')\n",
    "axes[0].set_title('Elbow Method - Optimal K Belirleme')\n",
    "axes[0].set_xlabel('Küme Sayısı (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "axes[1].set_xlabel('Küme Sayısı (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# En iyi k değerini seç\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"En iyi küme sayısı: {best_k} (Silhouette Score: {max(silhouette_scores):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means ile anomali tespiti\n",
    "print(f\"K-means clustering (k={best_k}) uygulanıyor...\")\n",
    "\n",
    "# Normal veriler üzerinde eğit\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "kmeans.fit(X_train_normal)\n",
    "\n",
    "# Test verisi üzerinde tahmin yap\n",
    "test_labels = kmeans.predict(X_test_sample)\n",
    "test_distances = kmeans.transform(X_test_sample)\n",
    "\n",
    "# Her nokta için en yakın küme merkezine olan mesafeyi hesapla\n",
    "min_distances = np.min(test_distances, axis=1)\n",
    "\n",
    "# Anomali skorunu hesapla (yüksek mesafe = yüksek anomali skoru)\n",
    "anomaly_scores_kmeans = min_distances\n",
    "\n",
    "# Threshold belirleme (95. percentile)\n",
    "threshold_kmeans = np.percentile(anomaly_scores_kmeans, 95)\n",
    "predictions_kmeans = (anomaly_scores_kmeans > threshold_kmeans).astype(int)\n",
    "\n",
    "print(f\"K-means threshold: {threshold_kmeans:.3f}\")\n",
    "print(f\"Tespit edilen anomali sayısı: {sum(predictions_kmeans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sonuçlarını görselleştir\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# PCA ile boyut indirgeme sonrası K-means eğitimi\n",
    "kmeans_pca = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "X_train_pca_normal = pca.transform(X_train_normal)\n",
    "kmeans_pca.fit(X_train_pca_normal)\n",
    "\n",
    "# Test verisi üzerinde tahmin yap (PCA)\n",
    "test_labels_pca = kmeans_pca.predict(X_test_pca)\n",
    "scatter = axes[0,0].scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=test_labels_pca, cmap='viridis', alpha=0.6)\n",
    "axes[0,0].set_title('K-means Kümeleme Sonuçları (PCA)')\n",
    "axes[0,0].set_xlabel('PC1')\n",
    "axes[0,0].set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=axes[0,0])\n",
    "\n",
    "# Anomali skorları dağılımı\n",
    "axes[0,1].hist(anomaly_scores_kmeans[y_test_sample == 0], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "axes[0,1].hist(anomaly_scores_kmeans[y_test_sample == 1], bins=50, alpha=0.7, label='Anomali', density=True)\n",
    "axes[0,1].axvline(threshold_kmeans, color='red', linestyle='--', label='Threshold')\n",
    "axes[0,1].set_title('K-means Anomali Skorları Dağılımı')\n",
    "axes[0,1].set_xlabel('Anomali Skoru')\n",
    "axes[0,1].set_ylabel('Yoğunluk')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Gerçek vs Tahmin edilen\n",
    "axes[1,0].scatter(X_test_pca[y_test_sample == 0, 0], X_test_pca[y_test_sample == 0, 1], \n",
    "                 alpha=0.6, label='Gerçek Normal', s=20, c='blue')\n",
    "axes[1,0].scatter(X_test_pca[y_test_sample == 1, 0], X_test_pca[y_test_sample == 1, 1], \n",
    "                 alpha=0.6, label='Gerçek Anomali', s=20, c='red')\n",
    "axes[1,0].scatter(X_test_pca[predictions_kmeans == 1, 0], X_test_pca[predictions_kmeans == 1, 1], \n",
    "                 alpha=0.8, label='Tahmin Anomali', s=50, c='orange', marker='x')\n",
    "axes[1,0].set_title('Gerçek vs Tahmin Edilen Anomaliler')\n",
    "axes[1,0].set_xlabel('PC1')\n",
    "axes[1,0].set_ylabel('PC2')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_sample, predictions_kmeans)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
    "axes[1,1].set_title('K-means Confusion Matrix')\n",
    "axes[1,1].set_xlabel('Tahmin Edilen')\n",
    "axes[1,1].set_ylabel('Gerçek')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN parametrelerini optimize et\n",
    "print(\"DBSCAN parametreleri optimize ediliyor...\")\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Optimal eps değerini bul (k-distance graph)\n",
    "sample_for_dbscan = X_train_normal[:2000]  # Hız için küçük sample\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(sample_for_dbscan)\n",
    "distances, indices = neighbors_fit.kneighbors(sample_for_dbscan)\n",
    "\n",
    "# 4. en yakın komşu mesafelerini sırala\n",
    "distances = np.sort(distances[:, 4], axis=0)\n",
    "\n",
    "# K-distance grafiği\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.title('K-distance Graph (k=4)')\n",
    "plt.xlabel('Veri Noktaları (sıralı)')\n",
    "plt.ylabel('4. En Yakın Komşu Mesafesi')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Elbow noktasını yaklaşık olarak bul\n",
    "knee_point = int(len(distances) * 0.95)  # %95'lik dilim\n",
    "optimal_eps = distances[knee_point]\n",
    "plt.axhline(y=optimal_eps, color='red', linestyle='--', label=f'Optimal eps ≈ {optimal_eps:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Önerilen eps değeri: {optimal_eps:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN ile anomali tespiti\n",
    "print(f\"DBSCAN clustering (eps={optimal_eps:.3f}) uygulanıyor...\")\n",
    "\n",
    "# DBSCAN'i normal veriler üzerinde eğit\n",
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=5)\n",
    "normal_labels = dbscan.fit_predict(sample_for_dbscan)\n",
    "\n",
    "# Küme sayısını ve noise noktalarını say\n",
    "n_clusters = len(set(normal_labels)) - (1 if -1 in normal_labels else 0)\n",
    "n_noise = list(normal_labels).count(-1)\n",
    "\n",
    "print(f\"Küme sayısı: {n_clusters}\")\n",
    "print(f\"Noise noktaları: {n_noise} ({n_noise/len(normal_labels)*100:.1f}%)\")\n",
    "\n",
    "# Test verisi için anomali tespiti\n",
    "# DBSCAN'de yeni noktalar için predict yok, mesafe tabanlı yaklaşım kullan\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Core noktaları bul\n",
    "core_samples_mask = np.zeros_like(normal_labels, dtype=bool)\n",
    "core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "core_samples = sample_for_dbscan[core_samples_mask]\n",
    "\n",
    "# Test noktaları için en yakın core sample'a olan mesafeyi hesapla\n",
    "if len(core_samples) > 0:\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(core_samples)\n",
    "    distances_to_core, _ = nn.kneighbors(X_test_sample)\n",
    "    anomaly_scores_dbscan = distances_to_core.flatten()\n",
    "    \n",
    "    # Threshold belirleme\n",
    "    threshold_dbscan = np.percentile(anomaly_scores_dbscan, 95)\n",
    "    predictions_dbscan = (anomaly_scores_dbscan > threshold_dbscan).astype(int)\n",
    "    \n",
    "    print(f\"DBSCAN threshold: {threshold_dbscan:.3f}\")\n",
    "    print(f\"Tespit edilen anomali sayısı: {sum(predictions_dbscan)}\")\n",
    "else:\n",
    "    print(\"DBSCAN core sample bulamadı, parametreleri ayarlayın.\")\n",
    "    anomaly_scores_dbscan = np.zeros(len(X_test_sample))\n",
    "    predictions_dbscan = np.zeros(len(X_test_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN sonuçlarını görselleştir\n",
    "if len(core_samples) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Normal veri kümeleme sonuçları (PCA)\n",
    "    sample_pca = pca.transform(sample_for_dbscan)\n",
    "    unique_labels = set(normal_labels)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            col = 'black'  # Noise için siyah\n",
    "        class_member_mask = (normal_labels == k)\n",
    "        xy = sample_pca[class_member_mask]\n",
    "        axes[0,0].scatter(xy[:, 0], xy[:, 1], c=[col], alpha=0.6, s=20)\n",
    "    \n",
    "    axes[0,0].set_title(f'DBSCAN Kümeleme (eps={optimal_eps:.2f})')\n",
    "    axes[0,0].set_xlabel('PC1')\n",
    "    axes[0,0].set_ylabel('PC2')\n",
    "    \n",
    "    # Anomali skorları dağılımı\n",
    "    axes[0,1].hist(anomaly_scores_dbscan[y_test_sample == 0], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "    axes[0,1].hist(anomaly_scores_dbscan[y_test_sample == 1], bins=50, alpha=0.7, label='Anomali', density=True)\n",
    "    axes[0,1].axvline(threshold_dbscan, color='red', linestyle='--', label='Threshold')\n",
    "    axes[0,1].set_title('DBSCAN Anomali Skorları Dağılımı')\n",
    "    axes[0,1].set_xlabel('Anomali Skoru')\n",
    "    axes[0,1].set_ylabel('Yoğunluk')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Gerçek vs Tahmin edilen\n",
    "    axes[1,0].scatter(X_test_pca[y_test_sample == 0, 0], X_test_pca[y_test_sample == 0, 1], \n",
    "                     alpha=0.6, label='Gerçek Normal', s=20, c='blue')\n",
    "    axes[1,0].scatter(X_test_pca[y_test_sample == 1, 0], X_test_pca[y_test_sample == 1, 1], \n",
    "                     alpha=0.6, label='Gerçek Anomali', s=20, c='red')\n",
    "    axes[1,0].scatter(X_test_pca[predictions_dbscan == 1, 0], X_test_pca[predictions_dbscan == 1, 1], \n",
    "                     alpha=0.8, label='Tahmin Anomali', s=50, c='orange', marker='x')\n",
    "    axes[1,0].set_title('DBSCAN: Gerçek vs Tahmin')\n",
    "    axes[1,0].set_xlabel('PC1')\n",
    "    axes[1,0].set_ylabel('PC2')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_sample, predictions_dbscan)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
    "    axes[1,1].set_title('DBSCAN Confusion Matrix')\n",
    "    axes[1,1].set_xlabel('Tahmin Edilen')\n",
    "    axes[1,1].set_ylabel('Gerçek')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest ile anomali tespiti\n",
    "print(\"Isolation Forest uygulanıyor...\")\n",
    "\n",
    "# Model oluştur ve eğit\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "iso_forest.fit(X_train_normal)\n",
    "\n",
    "# Test verisi üzerinde tahmin yap\n",
    "anomaly_scores_iso = iso_forest.decision_function(X_test_sample)\n",
    "predictions_iso = iso_forest.predict(X_test_sample)\n",
    "predictions_iso = (predictions_iso == -1).astype(int)  # -1: anomali, 1: normal\n",
    "\n",
    "print(f\"Tespit edilen anomali sayısı: {sum(predictions_iso)}\")\n",
    "print(f\"Anomali oranı: {sum(predictions_iso)/len(predictions_iso)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM ile anomali tespiti\n",
    "print(\"One-Class SVM uygulanıyor...\")\n",
    "\n",
    "# Model oluştur ve eğit (küçük sample ile)\n",
    "oc_svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')\n",
    "oc_svm.fit(X_train_normal[:5000])  # Hız için subset\n",
    "\n",
    "# Test verisi üzerinde tahmin yap\n",
    "anomaly_scores_svm = oc_svm.decision_function(X_test_sample)\n",
    "predictions_svm = oc_svm.predict(X_test_sample)\n",
    "predictions_svm = (predictions_svm == -1).astype(int)  # -1: anomali, 1: normal\n",
    "\n",
    "print(f\"Tespit edilen anomali sayısı: {sum(predictions_svm)}\")\n",
    "print(f\"Anomali oranı: {sum(predictions_svm)/len(predictions_svm)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOF ile anomali tespiti\n",
    "print(\"Local Outlier Factor (LOF) uygulanıyor...\")\n",
    "\n",
    "# LOF modeli (novelty=True ile yeni veriler için kullanılabilir)\n",
    "lof = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)\n",
    "lof.fit(X_train_normal[:5000])  # Hız için subset\n",
    "\n",
    "# Test verisi üzerinde tahmin yap\n",
    "anomaly_scores_lof = lof.decision_function(X_test_sample)\n",
    "predictions_lof = lof.predict(X_test_sample)\n",
    "predictions_lof = (predictions_lof == -1).astype(int)  # -1: anomali, 1: normal\n",
    "\n",
    "print(f\"Tespit edilen anomali sayısı: {sum(predictions_lof)}\")\n",
    "print(f\"Anomali oranı: {sum(predictions_lof)/len(predictions_lof)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Algoritma Karşılaştırması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm algoritmaların performansını karşılaştır\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "algorithms = {\n",
    "    'K-means': predictions_kmeans,\n",
    "    'DBSCAN': predictions_dbscan,\n",
    "    'Isolation Forest': predictions_iso,\n",
    "    'One-Class SVM': predictions_svm,\n",
    "    'LOF': predictions_lof\n",
    "}\n",
    "\n",
    "anomaly_scores = {\n",
    "    'K-means': anomaly_scores_kmeans,\n",
    "    'DBSCAN': anomaly_scores_dbscan,\n",
    "    'Isolation Forest': -anomaly_scores_iso,  # Negatif çünkü düşük skor = yüksek anomali\n",
    "    'One-Class SVM': -anomaly_scores_svm,\n",
    "    'LOF': -anomaly_scores_lof\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, pred in algorithms.items():\n",
    "    if len(pred) > 0 and not np.all(pred == 0):\n",
    "        accuracy = accuracy_score(y_test_sample, pred)\n",
    "        precision = precision_score(y_test_sample, pred, zero_division=0)\n",
    "        recall = recall_score(y_test_sample, pred, zero_division=0)\n",
    "        f1 = f1_score(y_test_sample, pred, zero_division=0)\n",
    "        \n",
    "        # ROC AUC için anomali skorlarını kullan\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test_sample, anomaly_scores[name])\n",
    "        except:\n",
    "            roc_auc = 0.5\n",
    "        \n",
    "        results.append({\n",
    "            'Algorithm': name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'Detected_Anomalies': sum(pred)\n",
    "        })\n",
    "\n",
    "# Sonuçları DataFrame'e çevir\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== Algoritma Karsilastirmasi ===\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performans metrikleri görselleştirmesi\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    if i < len(axes):\n",
    "        values = results_df[metric].values\n",
    "        algorithms_list = results_df['Algorithm'].values\n",
    "        \n",
    "        bars = axes[i].bar(range(len(algorithms_list)), values, alpha=0.8)\n",
    "        axes[i].set_title(f'{metric} Karşılaştırması')\n",
    "        axes[i].set_xlabel('Algoritma')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        axes[i].set_xticks(range(len(algorithms_list)))\n",
    "        axes[i].set_xticklabels(algorithms_list, rotation=45, ha='right')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Değerleri bar üzerine yaz\n",
    "        for bar, value in zip(bars, values):\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Son subplot'u kaldır\n",
    "if len(metrics) < len(axes):\n",
    "    fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En iyi algoritmaları belirle\n",
    "print(\"=== SONUCLAR VE DEGERLENDIRME ===\")\n",
    "print(\"1. PERFORMANS SIRALAMASI:\")\n",
    "\n",
    "# F1-Score'a göre sırala\n",
    "best_f1 = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "print(f\"   En İyi F1-Score: {best_f1['Algorithm']} ({best_f1['F1-Score']:.3f})\")\n",
    "\n",
    "# ROC-AUC'ye göre sırala\n",
    "best_auc = results_df.loc[results_df['ROC-AUC'].idxmax()]\n",
    "print(f\"   En İyi ROC-AUC: {best_auc['Algorithm']} ({best_auc['ROC-AUC']:.3f})\")\n",
    "\n",
    "# Precision'a göre sırala\n",
    "best_precision = results_df.loc[results_df['Precision'].idxmax()]\n",
    "print(f\"   En İyi Precision: {best_precision['Algorithm']} ({best_precision['Precision']:.3f})\")\n",
    "\n",
    "# Recall'a göre sırala\n",
    "best_recall = results_df.loc[results_df['Recall'].idxmax()]\n",
    "print(f\"   En İyi Recall: {best_recall['Algorithm']} ({best_recall['Recall']:.3f})\")\n",
    "\n",
    "print(\"2. GENEL DEĞERLENDİRME:\")\n",
    "print(\"   - Yüksek Precision: False positive'leri azaltır\")\n",
    "print(\"   - Yüksek Recall: Gerçek anomalileri kaçırmaz\")\n",
    "print(\"   - F1-Score: Precision ve Recall'un dengeli kombinasyonu\")\n",
    "print(\"   - ROC-AUC: Genel sınıflandırma performansı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ozet tablo\n",
    "print(\"=== OZET TABLO ===\")\n",
    "summary_df = results_df.copy()\n",
    "summary_df['Rank_F1'] = summary_df['F1-Score'].rank(ascending=False).astype(int)\n",
    "summary_df['Rank_AUC'] = summary_df['ROC-AUC'].rank(ascending=False).astype(int)\n",
    "summary_df['Overall_Rank'] = (summary_df['Rank_F1'] + summary_df['Rank_AUC']) / 2\n",
    "\n",
    "# Sırala\n",
    "summary_df = summary_df.sort_values('Overall_Rank')\n",
    "\n",
    "print(summary_df[['Algorithm', 'F1-Score', 'ROC-AUC', 'Precision', 'Recall', 'Overall_Rank']].round(3))\n",
    "\n",
    "print(\"=== ÖNERİLER ===\")\n",
    "print(\"1. En iyi genel performans için:\", summary_df.iloc[0]['Algorithm'])\n",
    "print(\"2. Yüksek hassasiyet için: Precision değeri en yüksek algoritma\")\n",
    "print(\"3. Anomali kaçırma riskini minimize etmek için: Recall değeri en yüksek algoritma\")\n",
    "print(\"4. Dengeli performans için: F1-Score değeri en yüksek algoritma\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
